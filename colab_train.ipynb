{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "A100"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# \ud83e\udde0 ExperimentalDLLM \u2014 Colab Training\n",
                "\n",
                "Resume pre-training of the Episodic-Centric PCModel from step 610,000.\n",
                "\n",
                "**Prerequisites:**\n",
                "1. Upload `ExperimentalDLLM_colab.tar.gz` to Google Drive root\n",
                "2. Use Colab Pro with GPU runtime (A100 preferred, T4 acceptable)\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. GPU Check"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import os\n",
                "\n",
                "if not torch.cuda.is_available():\n",
                "    raise RuntimeError(\n",
                "        \"\u274c No GPU detected!\\n\"\n",
                "        \"Go to Runtime \u2192 Change runtime type \u2192 GPU (A100 or T4)\"\n",
                "    )\n",
                "\n",
                "gpu_name = torch.cuda.get_device_name(0)\n",
                "gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
                "\n",
                "print(f\"\u2705 GPU: {gpu_name}\")\n",
                "print(f\"   Memory: {gpu_mem:.1f} GB\")\n",
                "print(f\"   CUDA: {torch.version.cuda}\")\n",
                "print(f\"   PyTorch: {torch.__version__}\")\n",
                "\n",
                "# Recommend batch size based on GPU\n",
                "if gpu_mem > 35:\n",
                "    RECOMMENDED_BATCH = 32\n",
                "    print(f\"\\n\ud83d\ude80 A100 detected \u2014 can use batch_size={RECOMMENDED_BATCH}\")\n",
                "elif gpu_mem > 14:\n",
                "    RECOMMENDED_BATCH = 16\n",
                "    print(f\"\\n\u26a1 Good GPU \u2014 recommended batch_size={RECOMMENDED_BATCH}\")\n",
                "else:\n",
                "    RECOMMENDED_BATCH = 8\n",
                "    print(f\"\\n\u26a0\ufe0f  Limited GPU \u2014 using batch_size={RECOMMENDED_BATCH}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Mount Google Drive & Extract Code"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import drive\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "# Extract the uploaded tarball\n",
                "DRIVE_TARBALL = '/content/drive/MyDrive/ExperimentalDLLM_colab.tar.gz'\n",
                "WORK_DIR = '/content/ExperimentalDLLM'\n",
                "\n",
                "if not os.path.exists(DRIVE_TARBALL):\n",
                "    raise FileNotFoundError(\n",
                "        f\"\u274c Tarball not found at {DRIVE_TARBALL}\\n\"\n",
                "        \"Please upload ExperimentalDLLM_colab.tar.gz to Google Drive root.\"\n",
                "    )\n",
                "\n",
                "print(f\">> Extracting {DRIVE_TARBALL}...\")\n",
                "!tar -xzf {DRIVE_TARBALL} -C /content/\n",
                "\n",
                "# Rename if needed\n",
                "extracted = '/content/ExperimentalDLLM_colab'\n",
                "if os.path.exists(extracted) and not os.path.exists(WORK_DIR):\n",
                "    os.rename(extracted, WORK_DIR)\n",
                "elif os.path.exists(extracted):\n",
                "    !cp -r {extracted}/* {WORK_DIR}/\n",
                "\n",
                "os.chdir(WORK_DIR)\n",
                "print(f\"\\n\u2705 Working directory: {os.getcwd()}\")\n",
                "print(f\"   Contents: {os.listdir('.')}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Install Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q transformers datasets tqdm matplotlib\n",
                "print(\"\u2705 Dependencies installed\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Verify Checkpoint\n",
                "\n",
                "Make sure the step 610,000 checkpoint is available."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "CKPT_DIR = os.path.join(WORK_DIR, 'checkpoints')\n",
                "CKPT_FILE = os.path.join(CKPT_DIR, 'checkpoint_small_pretrain_step610000.pt')\n",
                "\n",
                "# Set up Drive checkpoint directory for saving\n",
                "DRIVE_CKPT_DIR = '/content/drive/MyDrive/ExperimentalDLLM_checkpoints'\n",
                "os.makedirs(DRIVE_CKPT_DIR, exist_ok=True)\n",
                "\n",
                "if os.path.exists(CKPT_FILE):\n",
                "    size_gb = os.path.getsize(CKPT_FILE) / 1e9\n",
                "    print(f\"\u2705 Checkpoint found: {CKPT_FILE} ({size_gb:.1f} GB)\")\n",
                "else:\n",
                "    # Check Drive for previously saved checkpoints\n",
                "    drive_ckpts = sorted([\n",
                "        f for f in os.listdir(DRIVE_CKPT_DIR)\n",
                "        if f.startswith('checkpoint_small_pretrain_step') and f.endswith('.pt')\n",
                "        and 'latest' not in f\n",
                "    ]) if os.path.exists(DRIVE_CKPT_DIR) else []\n",
                "    \n",
                "    if drive_ckpts:\n",
                "        latest = drive_ckpts[-1]\n",
                "        print(f\">> Copying latest checkpoint from Drive: {latest}\")\n",
                "        os.makedirs(CKPT_DIR, exist_ok=True)\n",
                "        !cp \"{DRIVE_CKPT_DIR}/{latest}\" \"{CKPT_DIR}/\"\n",
                "        !cp \"{DRIVE_CKPT_DIR}/{latest}\" \"{CKPT_DIR}/checkpoint_small_pretrain_latest.pt\"\n",
                "        CKPT_FILE = os.path.join(CKPT_DIR, latest)\n",
                "        print(f\"\u2705 Checkpoint restored: {latest}\")\n",
                "    else:\n",
                "        print(\"\u26a0\ufe0f  No checkpoint found. Training will start from scratch.\")\n",
                "        CKPT_FILE = None\n",
                "\n",
                "print(f\"\\n\ud83d\udcc1 Drive checkpoint directory: {DRIVE_CKPT_DIR}\")\n",
                "print(f\"   New checkpoints will be saved here automatically.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Configure Training\n",
                "\n",
                "Adjust batch size for GPU memory. Data streams directly from HuggingFace \u2014 no caching needed."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# CONFIGURATION \u2014 Adjust these as needed\n",
                "# ============================================================\n",
                "\n",
                "BATCH_SIZE = RECOMMENDED_BATCH  # Auto-set by GPU (cell 1)\n",
                "TOTAL_STEPS = 5000000           # ~80B tokens at batch=32\n",
                "CHECKPOINT_INTERVAL = 5000      # Save every N steps\n",
                "LOG_INTERVAL = 100              # Print loss every N steps\n",
                "\n",
                "# ============================================================\n",
                "\n",
                "# Patch the config to use the optimal batch size\n",
                "from src.config import ConfigSmall\n",
                "original_bs = ConfigSmall.batch_size\n",
                "ConfigSmall.batch_size = BATCH_SIZE\n",
                "print(f\">> Batch size: {original_bs} \u2192 {BATCH_SIZE}\")\n",
                "print(f\">> Total steps: {TOTAL_STEPS:,}\")\n",
                "print(f\">> Checkpoint interval: {CHECKPOINT_INTERVAL:,}\")\n",
                "print(f\">> Data: Streaming from HuggingFace (no cache needed)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. \ud83d\ude80 Start Training\n",
                "\n",
                "Uses `--stream` to load data directly from HuggingFace FineWeb-Edu.\n",
                "Checkpoints auto-sync to Google Drive every 5 minutes."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import subprocess\n",
                "import shutil\n",
                "import signal\n",
                "import time\n",
                "\n",
                "# Build the training command\n",
                "cmd = [\n",
                "    'python', 'train_episodic.py',\n",
                "    '--config', 'small',\n",
                "    '--mode', 'pretrain',\n",
                "    '--steps', str(TOTAL_STEPS),\n",
                "    '--device', 'cuda',\n",
                "    '--stream',\n",
                "    '--save_dir', CKPT_DIR,\n",
                "    '--checkpoint_interval', str(CHECKPOINT_INTERVAL),\n",
                "    '--log_interval', str(LOG_INTERVAL),\n",
                "    '--resume', CKPT_DIR,\n",
                "]\n",
                "\n",
                "print(f\">> Command: {' '.join(cmd)}\")\n",
                "print(f\">> Checkpoints will be copied to Google Drive every {CHECKPOINT_INTERVAL} steps\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# Run training with real-time output\n",
                "process = subprocess.Popen(\n",
                "    cmd,\n",
                "    stdout=subprocess.PIPE,\n",
                "    stderr=subprocess.STDOUT,\n",
                "    universal_newlines=True,\n",
                "    bufsize=1\n",
                ")\n",
                "\n",
                "last_drive_sync = time.time()\n",
                "DRIVE_SYNC_INTERVAL = 300  # Sync to Drive every 5 minutes\n",
                "\n",
                "try:\n",
                "    for line in process.stdout:\n",
                "        print(line, end='')\n",
                "        \n",
                "        # Auto-sync checkpoints to Google Drive periodically\n",
                "        if '\ud83d\udcbe Checkpoint saved' in line and time.time() - last_drive_sync > DRIVE_SYNC_INTERVAL:\n",
                "            latest = os.path.join(CKPT_DIR, 'checkpoint_small_pretrain_latest.pt')\n",
                "            if os.path.exists(latest):\n",
                "                try:\n",
                "                    step_str = line.split('step')[1].split('\\u2192')[0].strip()\n",
                "                    step_file = f'checkpoint_small_pretrain_step{step_str}.pt'\n",
                "                    src = os.path.join(CKPT_DIR, step_file)\n",
                "                    if os.path.exists(src):\n",
                "                        shutil.copy2(src, DRIVE_CKPT_DIR)\n",
                "                        shutil.copy2(src, os.path.join(DRIVE_CKPT_DIR, 'checkpoint_small_pretrain_latest.pt'))\n",
                "                        print(f\"   \u2601\ufe0f  Synced to Google Drive: {step_file}\")\n",
                "                except:\n",
                "                    shutil.copy2(latest, DRIVE_CKPT_DIR)\n",
                "                    print(f\"   \u2601\ufe0f  Synced latest checkpoint to Google Drive\")\n",
                "                last_drive_sync = time.time()\n",
                "\n",
                "except KeyboardInterrupt:\n",
                "    print(\"\\n\\n\u26a0\ufe0f  Training interrupted! Saving final checkpoint...\")\n",
                "    process.send_signal(signal.SIGINT)\n",
                "    process.wait(timeout=30)\n",
                "\n",
                "finally:\n",
                "    latest = os.path.join(CKPT_DIR, 'checkpoint_small_pretrain_latest.pt')\n",
                "    if os.path.exists(latest):\n",
                "        shutil.copy2(latest, os.path.join(DRIVE_CKPT_DIR, 'checkpoint_small_pretrain_latest.pt'))\n",
                "        print(f\"\\n\u2601\ufe0f  Final checkpoint synced to Google Drive\")\n",
                "    \n",
                "    retcode = process.wait()\n",
                "    print(f\"\\nTraining process exited with code: {retcode}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Benchmark (Optional)\n",
                "\n",
                "Run the WikiText-2 perplexity benchmark on the latest checkpoint."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Find the latest checkpoint\n",
                "ckpts = sorted([\n",
                "    f for f in os.listdir(CKPT_DIR)\n",
                "    if f.startswith('checkpoint_small_pretrain_step') and f.endswith('.pt')\n",
                "    and 'latest' not in f\n",
                "])\n",
                "\n",
                "if ckpts:\n",
                "    latest_ckpt = os.path.join(CKPT_DIR, ckpts[-1])\n",
                "    print(f\">> Benchmarking: {ckpts[-1]}\")\n",
                "    !python benchmark_all.py --synapse_ckpt \"{latest_ckpt}\" --config small\n",
                "else:\n",
                "    print(\"\u274c No checkpoints found to benchmark.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## \ud83d\udd04 Session Recovery\n",
                "\n",
                "If your Colab session times out:\n",
                "\n",
                "1. **Start a new runtime** (Runtime \u2192 Change runtime type \u2192 GPU)\n",
                "2. **Run cells 1\u20134** \u2014 detects the latest checkpoint from Google Drive\n",
                "3. **Run cells 5\u20136** \u2014 training auto-resumes from where it left off\n",
                "\n",
                "Checkpoints are safe in `My Drive/ExperimentalDLLM_checkpoints/`"
            ]
        }
    ]
}