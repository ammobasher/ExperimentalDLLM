#!/usr/bin/env python3
"""
Training Progress Analyzer and Visualizer

Parses training logs and creates visualizations to analyze training progress.
"""

import matplotlib.pyplot as plt
import numpy as np
from pathlib import Path

# Training log data extracted from the run
# Format: (step, loss, ce_loss, pc_loss, time_seconds)
training_data = [
    (100, 2.7543, 2.7340, 0.2033, 184.24),
    (200, 6.4294, 6.3939, 0.3547, 185.21),
    (300, 4.9059, 4.8769, 0.2892, 182.31),
    (400, 4.1889, 4.1624, 0.2650, 183.67),
    (500, 2.4948, 2.4741, 0.2070, 204.01),
    (600, 4.3958, 4.3705, 0.2532, 190.22),
    (700, 6.3837, 6.3458, 0.3795, 213.73),
    (800, 4.6636, 4.6310, 0.3264, 195.73),
    (900, 6.1604, 6.1282, 0.3227, 189.36),
    (1000, 4.4367, 4.4110, 0.2575, 188.68),
    (1100, 4.0375, 4.0134, 0.2400, 193.28),
    (1200, 5.5212, 5.4896, 0.3157, 197.32),
    (1300, 2.6269, 2.6112, 0.1571, 190.96),
    (1400, 3.5681, 3.5379, 0.3015, 279.71),
    (1500, 3.8440, 3.8203, 0.2364, 192.84),
    (1600, 5.4558, 5.4150, 0.4081, 181.97),
    (1700, 5.8277, 5.7904, 0.3725, 188.53),
    (1800, 6.2443, 6.2087, 0.3558, 220.43),
    (1900, 4.1660, 4.1357, 0.3027, 196.07),
    (2000, 4.4034, 4.3757, 0.2765, 192.76),
    (2100, 5.6912, 5.6584, 0.3275, 224.32),
    (2200, 5.2257, 5.1972, 0.2851, 194.60),
    (2300, 5.8848, 5.8516, 0.3318, 198.63),
    (2400, 3.3497, 3.3304, 0.1935, 207.18),
    (2500, 4.3535, 4.3282, 0.2538, 193.32),
    (2600, 3.3860, 3.3670, 0.1904, 199.54),
    (2700, 2.1695, 2.1572, 0.1224, 188.68),
    (2800, 5.8353, 5.7880, 0.4732, 189.00),
    (2900, 2.7206, 2.7018, 0.1885, 190.89),
    (3000, 3.4668, 3.4461, 0.2072, 188.11),
    (3100, 2.9963, 2.9743, 0.2203, 187.66),
    (3200, 3.5043, 3.4784, 0.2592, 187.20),
    (3300, 4.3775, 4.3532, 0.2425, 187.44),
    (3400, 4.4053, 4.3787, 0.2666, 187.21),
    (3500, 6.3101, 6.2729, 0.3720, 187.27),
    (3600, 5.2063, 5.1744, 0.3186, 187.17),
    (3700, 4.7969, 4.7670, 0.2993, 187.31),
    (3800, 3.5842, 3.5559, 0.2830, 187.36),
    (3900, 3.6270, 3.6064, 0.2063, 187.98),
    (4000, 3.7338, 3.7110, 0.2275, 187.95),
    (4100, 5.0226, 4.9859, 0.3675, 187.49),
    (4200, 3.4660, 3.4436, 0.2244, 188.29),
    (4300, 3.8417, 3.8177, 0.2396, 187.51),
    (4400, 5.8932, 5.8519, 0.4127, 188.13),
    (4500, 4.5199, 4.4902, 0.2971, 187.36),
    (4600, 4.3659, 4.3349, 0.3093, 187.83),
    (4700, 3.9072, 3.8831, 0.2407, 187.88),
    (4800, 3.8812, 3.8495, 0.3166, 187.52),
    (4900, 3.8880, 3.8631, 0.2486, 187.54),
    (5000, 3.4978, 3.4733, 0.2457, 187.54),
    (5100, 5.4931, 5.4550, 0.3803, 187.66),
    (5200, 5.5951, 5.5516, 0.4349, 187.42),
    (5300, 3.8814, 3.8540, 0.2733, 187.08),
    (5400, 5.7899, 5.7506, 0.3923, 187.38),
    (5500, 5.2549, 5.2212, 0.3373, 187.94),
    (5600, 4.1305, 4.0987, 0.3177, 188.18),
    (5700, 3.7520, 3.7233, 0.2871, 187.73),
    (5800, 2.7976, 2.7782, 0.1943, 188.28),
    (5900, 3.4462, 3.4167, 0.2948, 187.49),
    (6000, 4.3838, 4.3437, 0.4010, 188.45),
    (6100, 4.2333, 4.2052, 0.2809, 187.77),
    (6200, 6.3470, 6.2999, 0.4712, 187.82),
    (6300, 2.9315, 2.9087, 0.2272, 188.28),
    (6400, 5.2728, 5.2322, 0.4060, 187.17),
    (6500, 3.1937, 3.1708, 0.2292, 187.69),
    (6600, 2.2522, 2.2364, 0.1574, 187.52),
    (6700, 3.9150, 3.8881, 0.2686, 188.02),
    (6800, 4.8583, 4.8164, 0.4186, 187.82),
    (6900, 5.6659, 5.6199, 0.4597, 188.27),
    (7000, 5.3940, 5.3478, 0.4618, 188.05),
    (7100, 3.3155, 3.2867, 0.2878, 188.19),
    (7200, 3.2598, 3.2347, 0.2514, 188.34),
    (7300, 4.3643, 4.3271, 0.3717, 188.19),
    (7400, 2.3369, 2.3096, 0.2728, 188.45),
    (7500, 5.0671, 5.0264, 0.4067, 187.74),
    (7600, 5.0667, 5.0256, 0.4110, 188.66),
    (7700, 4.1189, 4.0884, 0.3056, 188.28),
    (7800, 5.1036, 5.0623, 0.4136, 188.70),
    (7900, 4.6245, 4.5852, 0.3927, 188.82),
    (8000, 3.7532, 3.7194, 0.3376, 188.29),
    (8100, 5.6280, 5.5741, 0.5391, 188.52),
    (8200, 4.2325, 4.1945, 0.3799, 188.55),
    (8300, 3.6018, 3.5727, 0.2911, 187.22),
    (8400, 5.7357, 5.6936, 0.4210, 187.67),
    (8500, 3.4258, 3.3984, 0.2741, 188.18),
    (8600, 4.9558, 4.9188, 0.3695, 187.95),
    (8700, 4.5330, 4.4934, 0.3951, 188.40),
    (8800, 5.0203, 4.9722, 0.4807, 188.98),
    (8900, 2.8090, 2.7817, 0.2735, 188.36),
    (9000, 4.3975, 4.3642, 0.3336, 189.64),
    (9100, 4.6954, 4.6548, 0.4066, 189.31),
    (9200, 4.7126, 4.6550, 0.5764, 189.40),
    (9300, 2.9558, 2.9316, 0.2428, 189.72),
    (9400, 4.7461, 4.7071, 0.3898, 188.54),
    (9500, 2.6893, 2.6666, 0.2272, 191.26),
    (9600, 4.1093, 4.0742, 0.3512, 186.60),
    (9700, 6.0774, 6.0294, 0.4807, 187.59),
    (9800, 2.5410, 2.5186, 0.2241, 186.83),
    (9900, 5.4781, 5.4228, 0.5529, 187.58),
    (10000, 5.0475, 4.9985, 0.4897, 188.44),
    (10100, 4.0244, 3.9893, 0.3517, 189.31),
    (10200, 4.7427, 4.6999, 0.4277, 187.23),
    (10300, 4.9155, 4.8695, 0.4600, 187.23),
    (10400, 4.4223, 4.3796, 0.4261, 187.46),
    (10500, 3.8765, 3.8388, 0.3768, 188.37),
    (10600, 1.9843, 1.9657, 0.1859, 186.88),
    (10700, 4.1472, 4.1095, 0.3775, 187.82),
    (10800, 5.1780, 5.1343, 0.4367, 187.76),
    (10900, 4.2887, 4.2489, 0.3980, 188.44),
    (11000, 3.2235, 3.1894, 0.3407, 187.36),
    (11100, 2.6962, 2.6707, 0.2551, 187.36),
    (11200, 1.8569, 1.8310, 0.2596, 187.40),
    (11300, 2.7516, 2.7245, 0.2710, 188.44),
    (11400, 3.6161, 3.5803, 0.3586, 188.36),
    (11500, 4.9581, 4.9073, 0.5078, 187.75),
    (11600, 5.3885, 5.3354, 0.5308, 187.24),
    (11700, 3.8922, 3.8585, 0.3373, 188.26),
    (11800, 5.7129, 5.6591, 0.5387, 187.79),
    (11900, 5.8173, 5.7640, 0.5334, 187.78),
    (12000, 4.8043, 4.7567, 0.4758, 187.51),
    (12100, 4.9824, 4.9337, 0.4862, 188.09),
    (12200, 3.2703, 3.2346, 0.3574, 188.48),
    (12300, 3.8443, 3.8067, 0.3762, 187.90),
    (12400, 4.0823, 4.0451, 0.3723, 188.06),
    (12500, 3.3314, 3.2991, 0.3229, 188.13),
    (12600, 3.4692, 3.4368, 0.3244, 187.80),
    (12700, 3.0904, 3.0625, 0.2794, 188.30),
    (12800, 4.9044, 4.8524, 0.5199, 187.34),
    (12900, 4.1418, 4.0972, 0.4456, 187.61),
    (13000, 3.6860, 3.6491, 0.3684, 187.56),
    (13100, 5.5159, 5.4565, 0.5946, 188.13),
    (13200, 4.5884, 4.5362, 0.5226, 187.76),
    (13300, 4.8722, 4.8247, 0.4745, 187.83),
    (13400, 4.5356, 4.4868, 0.4881, 187.95),
    (13500, 3.8492, 3.8094, 0.3974, 187.01),
    (13600, 4.4405, 4.3973, 0.4320, 188.08),
    (13700, 4.2077, 4.1647, 0.4298, 187.43),
    (13800, 3.9382, 3.8964, 0.4172, 187.95),
    (13900, 4.0332, 3.9914, 0.4179, 187.55),
    (14000, 4.4441, 4.4015, 0.4252, 186.12),
    (14100, 3.7564, 3.7136, 0.4289, 187.61),
    (14200, 4.4061, 4.3601, 0.4600, 186.81),
    (14300, 4.3310, 4.2872, 0.4379, 187.83),
    (14400, 4.5703, 4.5225, 0.4777, 187.46),
    (14500, 3.5183, 3.4759, 0.4247, 187.34),
    (14600, 3.8295, 3.7902, 0.3928, 187.63),
    (14700, 3.7917, 3.7513, 0.4040, 187.41),
    (14800, 2.7488, 2.7168, 0.3201, 188.17),
    (14900, 2.6279, 2.5999, 0.2801, 187.59),
    (15000, 4.2560, 4.2066, 0.4940, 187.79),
    (15100, 3.8797, 3.8320, 0.4766, 187.32),
    (15200, 3.8113, 3.7686, 0.4278, 187.35),
    (15300, 3.7245, 3.6800, 0.4444, 187.49),
    (15400, 4.1096, 4.0648, 0.4482, 187.70),
    (15500, 2.9520, 2.9189, 0.3319, 187.52),
    (15600, 4.4899, 4.4389, 0.5098, 188.11),
    (15700, 4.1233, 4.0725, 0.5075, 187.89),
    (15800, 3.9603, 3.9171, 0.4318, 187.82),
    (15900, 4.8553, 4.8067, 0.4860, 188.11),
    (16000, 3.2919, 3.2607, 0.3118, 187.55),
    (16100, 4.8476, 4.7938, 0.5377, 188.00),
    (16200, 3.0632, 3.0260, 0.3718, 187.52),
    (16300, 2.8087, 2.7801, 0.2860, 187.88),
    (16400, 5.6413, 5.5787, 0.6269, 187.30),
    (16500, 3.0492, 3.0146, 0.3458, 188.48),
    (16600, 4.3138, 4.2644, 0.4933, 187.79),
    (16700, 4.2498, 4.2004, 0.4934, 187.54),
    (16800, 3.9419, 3.8988, 0.4312, 188.20),
    (16900, 5.0817, 5.0215, 0.6020, 186.64),
    (17000, 2.5847, 2.5555, 0.2915, 188.19),
    (17100, 5.2498, 5.1885, 0.6135, 188.26),
    (17200, 4.1227, 4.0801, 0.4262, 187.76),
    (17300, 4.3855, 4.3346, 0.5096, 187.92),
    (17400, 3.0233, 2.9847, 0.3856, 187.97),
    (17500, 4.4391, 4.3835, 0.5558, 189.33),
    (17600, 5.3140, 5.2474, 0.6667, 187.49),
    (17700, 5.6465, 5.5847, 0.6189, 189.35),
    (17800, 3.6896, 3.6498, 0.3979, 187.86),
    (17900, 4.9447, 4.8867, 0.5799, 187.80),
    (18000, 5.4170, 5.3534, 0.6359, 187.28),
    (18100, 1.5183, 1.4985, 0.1986, 188.46),
    (18200, 4.4991, 4.4474, 0.5168, 192.11),
    (18300, 3.4205, 3.3727, 0.4774, 221.54),
    (18400, 2.2844, 2.2565, 0.2792, 191.03),
    (18500, 3.5359, 3.4892, 0.4670, 205.03),
    (18600, 4.1913, 4.1396, 0.5169, 188.84),
    (18700, 3.8010, 3.7441, 0.5693, 188.05),
    (18800, 2.9567, 2.9205, 0.3618, 189.07),
    (18900, 3.3919, 3.3534, 0.3846, 189.04),
    (19000, 3.8243, 3.7782, 0.4610, 189.83),
    (19100, 4.5092, 4.4567, 0.5251, 188.95),
    (19200, 3.0381, 3.0023, 0.3578, 188.83),
    (19300, 3.6244, 3.5824, 0.4203, 189.09),
    (19400, 3.8111, 3.7677, 0.4341, 189.38),
    (19500, 3.6797, 3.6306, 0.4903, 189.75),
    (19600, 3.1606, 3.1181, 0.4258, 188.09),
    (19700, 3.5961, 3.5409, 0.5514, 188.84),
    (19800, 3.2478, 3.2047, 0.4310, 188.61),
    (19900, 3.0806, 3.0450, 0.3566, 188.83),
    (20000, 4.7184, 4.6593, 0.5917, 189.28),
    (20100, 4.2931, 4.2415, 0.5157, 192.64),
    (20200, 5.6754, 5.6081, 0.6735, 189.56),
    (20300, 3.7444, 3.6982, 0.4626, 188.59),
    (20400, 3.2528, 3.2125, 0.4031, 189.19),
    (20500, 3.6757, 3.6316, 0.4409, 186.64),
    (20600, 5.0458, 4.9796, 0.6621, 187.30),
    (20700, 3.7212, 3.6781, 0.4315, 186.74),
    (20800, 2.7941, 2.7586, 0.3550, 187.11),
    (20900, 5.2791, 5.2144, 0.6470, 187.43),
    (21000, 3.5215, 3.4749, 0.4661, 187.44),
    (21100, 4.1720, 4.1165, 0.5545, 187.27),
    (21200, 2.2958, 2.2584, 0.3743, 187.52),
    (21300, 3.2348, 3.1987, 0.3618, 187.97),
    (21400, 4.1709, 4.1162, 0.5467, 187.48),
    (21500, 5.2458, 5.1838, 0.6200, 187.29),
    (21600, 4.2751, 4.2210, 0.5409, 188.07),
    (21700, 3.5733, 3.5315, 0.4181, 188.04),
    (21800, 2.1391, 2.1126, 0.2653, 187.99),
    (21900, 5.1497, 5.0840, 0.6568, 187.88),
    (22000, 3.6850, 3.6399, 0.4509, 187.66),
    (22100, 4.3088, 4.2545, 0.5434, 189.94),
    (22200, 3.8885, 3.8335, 0.5501, 190.55),
    (22300, 4.9889, 4.9205, 0.6839, 198.87),
    (22400, 3.1664, 3.1244, 0.4208, 195.65),
    (22500, 3.7207, 3.6744, 0.4623, 194.72),
    (22600, 4.8110, 4.7507, 0.6031, 191.11),
    (22700, 4.2520, 4.1987, 0.5331, 189.88),
    (22800, 4.2827, 4.2252, 0.5750, 189.74),
    (22900, 2.0275, 2.0025, 0.2500, 189.96),
    (23000, 2.3745, 2.3474, 0.2712, 191.45),
    (23100, 4.8467, 4.7904, 0.5631, 190.65),
    (23200, 2.0796, 2.0553, 0.2437, 191.25),
    (23300, 4.3459, 4.2881, 0.5778, 190.43),
    (23400, 4.2649, 4.2123, 0.5258, 189.35),
    (23500, 2.8309, 2.7968, 0.3410, 195.23),
    (23600, 3.4132, 3.3705, 0.4269, 188.10),
    (23700, 3.2531, 3.2125, 0.4061, 188.98),
    (23800, 3.2470, 3.2061, 0.4088, 189.80),
    (23900, 2.6382, 2.6069, 0.3126, 188.29),
    (24000, 4.5881, 4.5263, 0.6178, 189.46),
    (24100, 2.6973, 2.6571, 0.4020, 188.32),
    (24200, 3.6508, 3.6052, 0.4553, 188.74),
    (24300, 3.7740, 3.7241, 0.4988, 188.19),
    (24400, 3.3825, 3.3291, 0.5340, 188.50),
    (24500, 2.5541, 2.5238, 0.3036, 189.00),
    (24600, 4.0639, 4.0074, 0.5652, 188.64),
    (24700, 3.9712, 3.9218, 0.4940, 188.36),
    (24800, 3.8201, 3.7742, 0.4594, 188.68),
    (24900, 4.4076, 4.3543, 0.5327, 190.85),
    (25000, 3.4588, 3.4172, 0.4160, 188.77),
    (25100, 1.7646, 1.7386, 0.2603, 190.31),
    (25200, 3.3823, 3.3303, 0.5196, 189.40),
    (25300, 3.2655, 3.2251, 0.4036, 189.04),
    (25400, 4.2591, 4.2062, 0.5292, 189.05),
    (25500, 3.9218, 3.8639, 0.5786, 189.66),
    (25600, 5.0531, 4.9882, 0.6489, 187.52),
    (25700, 3.5989, 3.5502, 0.4869, 188.96),
    (25800, 5.2460, 5.1773, 0.6875, 188.65),
    (25900, 4.3251, 4.2633, 0.6184, 188.22),
    (26000, 5.1054, 5.0357, 0.6971, 189.29),
    (26100, 3.4747, 3.4315, 0.4321, 188.93),
    (26200, 4.3631, 4.3077, 0.5540, 188.74),
    (26300, 5.0266, 4.9633, 0.6326, 188.73),
    (26400, 2.5570, 2.5176, 0.3941, 187.93),
    (26500, 3.9055, 3.8549, 0.5053, 189.20),
    (26600, 4.5540, 4.4858, 0.6822, 188.46),
    (26700, 4.0580, 4.0029, 0.5511, 188.17),
    (26800, 3.0167, 2.9751, 0.4164, 188.77),
    (26900, 3.3657, 3.3162, 0.4954, 188.69),
    (27000, 4.7959, 4.7344, 0.6143, 188.03),
    (27100, 3.9219, 3.8723, 0.4951, 188.53),
    (27200, 2.0010, 1.9758, 0.2522, 188.95),
    (27300, 3.8482, 3.7956, 0.5265, 188.55),
]

def analyze_training():
    """Analyze training progress and create visualizations."""
    
    # Convert to numpy arrays
    data = np.array(training_data)
    steps = data[:, 0]
    loss = data[:, 1]
    ce_loss = data[:, 2]
    pc_loss = data[:, 3]
    time_per_step = data[:, 4]
    
    # Create figure with subplots
    fig = plt.figure(figsize=(16, 12))
    fig.suptitle('ECLM Training Progress Analysis (27,300 / 500,000 steps)', fontsize=16, fontweight='bold')
    
    # Plot 1: Loss curves
    ax1 = fig.add_subplot(2, 2, 1)
    ax1.plot(steps, loss, 'b-', alpha=0.3, label='Total Loss')
    ax1.plot(steps, ce_loss, 'g-', alpha=0.3, label='CE Loss')
    
    # Add smoothed trend lines (moving average)
    window = 20
    loss_smooth = np.convolve(loss, np.ones(window)/window, mode='valid')
    ce_smooth = np.convolve(ce_loss, np.ones(window)/window, mode='valid')
    steps_smooth = steps[window-1:]
    
    ax1.plot(steps_smooth, loss_smooth, 'b-', linewidth=2, label='Total Loss (smoothed)')
    ax1.plot(steps_smooth, ce_smooth, 'g-', linewidth=2, label='CE Loss (smoothed)')
    
    ax1.set_xlabel('Training Steps')
    ax1.set_ylabel('Loss')
    ax1.set_title('Cross-Entropy Loss Over Time')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # Plot 2: PC Loss trend
    ax2 = fig.add_subplot(2, 2, 2)
    ax2.plot(steps, pc_loss, 'r-', alpha=0.3, label='PC Loss')
    
    # Smoothed PC loss
    pc_smooth = np.convolve(pc_loss, np.ones(window)/window, mode='valid')
    ax2.plot(steps_smooth, pc_smooth, 'r-', linewidth=2, label='PC Loss (smoothed)')
    
    ax2.set_xlabel('Training Steps')
    ax2.set_ylabel('Predictive Coding Loss')
    ax2.set_title('Predictive Coding Loss Over Time')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # Plot 3: Training speed
    ax3 = fig.add_subplot(2, 2, 3)
    ax3.plot(steps, time_per_step, 'purple', alpha=0.5)
    ax3.axhline(y=np.mean(time_per_step), color='red', linestyle='--', 
                label=f'Mean: {np.mean(time_per_step):.1f}s')
    ax3.set_xlabel('Training Steps')
    ax3.set_ylabel('Seconds per 100 steps')
    ax3.set_title('Training Speed (CPU)')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # Plot 4: Loss distribution histograms
    ax4 = fig.add_subplot(2, 2, 4)
    
    # Split into early, middle, late phases
    n_points = len(loss)
    early = loss[:n_points//3]
    middle = loss[n_points//3:2*n_points//3]
    late = loss[2*n_points//3:]
    
    ax4.hist(early, bins=20, alpha=0.5, label=f'Early (Î¼={np.mean(early):.2f})', color='red')
    ax4.hist(middle, bins=20, alpha=0.5, label=f'Middle (Î¼={np.mean(middle):.2f})', color='orange')
    ax4.hist(late, bins=20, alpha=0.5, label=f'Late (Î¼={np.mean(late):.2f})', color='green')
    
    ax4.set_xlabel('Loss Value')
    ax4.set_ylabel('Frequency')
    ax4.set_title('Loss Distribution by Training Phase')
    ax4.legend()
    ax4.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('training_analysis.png', dpi=150, bbox_inches='tight')
    print("âœ“ Saved training_analysis.png")
    
    # Print statistics
    print("\n" + "="*60)
    print("TRAINING STATISTICS")
    print("="*60)
    
    print(f"\nðŸ“Š LOSS ANALYSIS:")
    print(f"   CE Loss: {ce_loss[0]:.2f} â†’ {ce_loss[-1]:.2f} (Î” = {ce_loss[-1] - ce_loss[0]:+.2f})")
    print(f"   PC Loss: {pc_loss[0]:.2f} â†’ {pc_loss[-1]:.2f} (Î” = {pc_loss[-1] - pc_loss[0]:+.2f})")
    print(f"   Best CE Loss: {np.min(ce_loss):.2f} (step {int(steps[np.argmin(ce_loss)])})")
    print(f"   Worst CE Loss: {np.max(ce_loss):.2f} (step {int(steps[np.argmax(ce_loss)])})")
    
    print(f"\nðŸ“ˆ TREND ANALYSIS (smoothed):")
    print(f"   Early avg CE: {np.mean(ce_loss[:n_points//3]):.2f}")
    print(f"   Middle avg CE: {np.mean(ce_loss[n_points//3:2*n_points//3]):.2f}")
    print(f"   Late avg CE: {np.mean(ce_loss[2*n_points//3:]):.2f}")
    
    ce_trend = np.mean(ce_loss[2*n_points//3:]) - np.mean(ce_loss[:n_points//3])
    trend_direction = "â†“ DECREASING" if ce_trend < 0 else "â†‘ INCREASING"
    print(f"   Overall trend: {trend_direction} ({ce_trend:+.2f})")
    
    print(f"\nâ±ï¸  SPEED ANALYSIS:")
    avg_time = np.mean(time_per_step)
    print(f"   Avg time per 100 steps: {avg_time:.1f}s")
    print(f"   Avg time per step: {avg_time/100:.2f}s")
    
    # Time estimates
    remaining_steps = 500000 - 27300
    estimated_hours_cpu = (remaining_steps * avg_time / 100) / 3600
    estimated_days_cpu = estimated_hours_cpu / 24
    
    print(f"\nðŸ“… TIME ESTIMATES (CPU):")
    print(f"   Remaining steps: {remaining_steps:,}")
    print(f"   Estimated time: {estimated_hours_cpu:.0f} hours ({estimated_days_cpu:.0f} days)")
    
    # MPS estimate (assume ~50x speedup)
    mps_speedup = 50
    estimated_hours_mps = estimated_hours_cpu / mps_speedup
    print(f"\nðŸš€ WITH MPS (~{mps_speedup}x speedup):")
    print(f"   Estimated time: {estimated_hours_mps:.0f} hours ({estimated_hours_mps/24:.1f} days)")
    
    return fig


if __name__ == "__main__":
    analyze_training()
    print("\nâœ“ Analysis complete!")
