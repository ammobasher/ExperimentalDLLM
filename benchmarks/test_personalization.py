"""
Personalization Benchmarks for Episodic-Centric LLM.

Tests the key novel contributions:
1. Instant Adaptation: Can model adapt after single interaction?
2. Zero Forgetting: Does model retain general knowledge after many interactions?
3. Memory Efficiency: How much storage per accuracy improvement?
4. Sleep Quality: Does consolidation improve without forgetting?
"""

import torch
import torch.nn as nn
import numpy as np
from pathlib import Path
import sys
import time

# Add parent directory to path
sys.path.append(str(Path(__file__).parent.parent))

from src.model import PCModel
from src.config import ConfigSmall
from src.memory_optimized import OptimizedEpisodicMemory
from src.memory_generate import MemoryAugmentedModel, compute_surprise
from src.sleep import SleepConsolidation


def test_instant_adaptation(model, memory, config, device):
    """
    Test 1: Instant Adaptation

    Can the model instantly adapt after a single interaction?

    Expected: >15% improvement on user-specific queries after one interaction.
    """
    print("\n" + "="*60)
    print("TEST 1: INSTANT ADAPTATION")
    print("="*60)

    mem_model = MemoryAugmentedModel(model, memory, config)

    # Create a user-specific fact
    user_fact = "My favorite programming language is Rust"

    # Tokenize (simple placeholder - would use real tokenizer)
    fact_ids = torch.randint(0, config.vocab_size, (1, 20), device=device)

    # Force add to memory
    added = mem_model.add_memory(fact_ids, force=True)
    assert added, "Failed to add memory"

    print(f"‚úì Added user fact to memory")
    print(f"Memory count: {memory.count}")

    # Test retrieval
    query_ids = torch.randint(0, config.vocab_size, (1, 15), device=device)

    # Without memory (baseline)
    with torch.no_grad():
        logits_without, _ = model(query_ids, inference=True)
        baseline_entropy = -torch.softmax(logits_without[:, -1, :], dim=-1).max().item()

    # With memory
    generated, stats = mem_model.generate(query_ids, max_length=50, verbose=True)

    print(f"\n‚úì Generated with memory augmentation")
    print(f"Memories retrieved: {stats['memories_retrieved']}")
    print(f"Avg similarity: {stats['avg_similarity']:.3f}")
    print(f"Generation time: {stats['generation_time']:.2f}s")

    # Success criteria: Memory was retrieved and used
    success = stats['memories_retrieved'] > 0
    if success:
        print(f"\n‚úÖ INSTANT ADAPTATION: PASS")
        print(f"   Model successfully retrieved and used episodic memory")
    else:
        print(f"\n‚ùå INSTANT ADAPTATION: FAIL")
        print(f"   No memories were retrieved")

    return {
        'test': 'instant_adaptation',
        'passed': success,
        'memories_retrieved': stats['memories_retrieved'],
        'avg_similarity': stats['avg_similarity'],
    }


def test_zero_forgetting(model, memory, config, device, n_interactions=1000):
    """
    Test 2: Zero Catastrophic Forgetting

    Does the model forget pre-trained knowledge after many interactions?

    Expected: <2% degradation on general knowledge after 1000 interactions.
    """
    print("\n" + "="*60)
    print("TEST 2: ZERO CATASTROPHIC FORGETTING")
    print("="*60)

    # Baseline: Evaluate on general knowledge (placeholder)
    print("Measuring baseline performance...")

    baseline_losses = []
    for _ in range(50):
        batch = torch.randint(0, config.vocab_size, (1, config.seq_len), device=device)
        with torch.no_grad():
            logits, pc_loss = model(batch, inference=False)

            # Compute CE loss
            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = batch[..., 1:].contiguous()
            ce_loss = nn.CrossEntropyLoss()(
                shift_logits.view(-1, config.vocab_size),
                shift_labels.view(-1)
            )
            baseline_losses.append(ce_loss.item())

    baseline_loss = np.mean(baseline_losses)
    print(f"Baseline loss: {baseline_loss:.4f}")

    # Simulate N interactions (add to memory)
    print(f"\nSimulating {n_interactions} user interactions...")
    mem_model = MemoryAugmentedModel(model, memory, config)

    added_count = 0
    for i in range(n_interactions):
        batch = torch.randint(0, config.vocab_size, (1, 50), device=device)
        surprise = compute_surprise(model, batch)

        # Add to memory
        if mem_model.add_memory(batch):
            added_count += 1

        if (i + 1) % 200 == 0:
            print(f"  Progress: {i+1}/{n_interactions}, Memories: {memory.count}, Added: {added_count}")

    print(f"‚úì Completed {n_interactions} interactions")
    print(f"  Memories added: {added_count}")
    print(f"  Memory usage: {memory.count}/{memory.capacity} ({memory.count/memory.capacity*100:.1f}%)")

    # Re-evaluate on same general knowledge test
    print("\nRe-measuring performance after interactions...")

    after_losses = []
    for _ in range(50):
        batch = torch.randint(0, config.vocab_size, (1, config.seq_len), device=device)
        with torch.no_grad():
            logits, pc_loss = model(batch, inference=False)

            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = batch[..., 1:].contiguous()
            ce_loss = nn.CrossEntropyLoss()(
                shift_logits.view(-1, config.vocab_size),
                shift_labels.view(-1)
            )
            after_losses.append(ce_loss.item())

    after_loss = np.mean(after_losses)
    degradation = (after_loss - baseline_loss) / baseline_loss

    print(f"After loss: {after_loss:.4f}")
    print(f"Degradation: {degradation*100:.2f}%")

    # Success criteria: <2% degradation
    success = abs(degradation) < 0.02
    if success:
        print(f"\n‚úÖ ZERO FORGETTING: PASS")
        print(f"   Model retained general knowledge (degradation: {degradation*100:.2f}%)")
    else:
        print(f"\n‚ùå ZERO FORGETTING: FAIL")
        print(f"   Significant degradation observed: {degradation*100:.2f}%")

    return {
        'test': 'zero_forgetting',
        'passed': success,
        'baseline_loss': baseline_loss,
        'after_loss': after_loss,
        'degradation_percent': degradation * 100,
        'interactions': n_interactions,
        'memories_added': added_count,
    }


def test_memory_efficiency(model, memory, config, device):
    """
    Test 3: Memory Efficiency

    How much storage is required per accuracy improvement?

    Expected: <1MB per 1% accuracy improvement.
    """
    print("\n" + "="*60)
    print("TEST 3: MEMORY EFFICIENCY")
    print("="*60)

    # Memory storage
    memory_size_mb = (memory.count * config.memory_dim * 4) / (1024 * 1024)  # 4 bytes per float32

    print(f"Memory count: {memory.count}")
    print(f"Memory size: {memory_size_mb:.2f} MB")
    print(f"Per-memory size: {memory_size_mb / max(memory.count, 1) * 1000:.2f} KB")

    # Efficiency metric (placeholder - would need real accuracy measurements)
    efficiency_score = memory_size_mb / max(memory.count, 1)

    print(f"\nEfficiency: {efficiency_score:.4f} MB per memory")

    # Success criteria: Reasonable efficiency
    success = memory_size_mb < 100  # Less than 100MB for 50K memories
    if success:
        print(f"\n‚úÖ MEMORY EFFICIENCY: PASS")
        print(f"   Total memory usage: {memory_size_mb:.2f} MB")
    else:
        print(f"\n‚ùå MEMORY EFFICIENCY: FAIL")
        print(f"   Memory usage too high: {memory_size_mb:.2f} MB")

    return {
        'test': 'memory_efficiency',
        'passed': success,
        'memory_count': memory.count,
        'memory_size_mb': memory_size_mb,
        'per_memory_kb': memory_size_mb / max(memory.count, 1) * 1000,
    }


def test_sleep_consolidation(model, memory, config, device):
    """
    Test 4: Sleep Consolidation Quality

    Does sleep consolidation improve the model without causing forgetting?

    Expected: Successful consolidation with memory reduction and no forgetting.
    """
    print("\n" + "="*60)
    print("TEST 4: SLEEP CONSOLIDATION")
    print("="*60)

    # Add enough memories to trigger consolidation
    print("Adding memories to fill capacity...")
    target_count = int(memory.capacity * 0.85)  # Fill to 85%

    while memory.count < target_count:
        batch = torch.randint(0, config.vocab_size, (1, 50), device=device)
        surprise = compute_surprise(model, batch)
        embedding = model.encode(batch)
        metadata = {'embedding': embedding.cpu().numpy()}
        memory.add(embedding.cpu().numpy(), metadata, surprise + memory.threshold_tau + 1.0)

        if memory.count % 1000 == 0:
            print(f"  Progress: {memory.count}/{target_count}")

    print(f"‚úì Memory filled: {memory.count}/{memory.capacity}")

    # Measure before consolidation
    before_count = memory.count

    # Run sleep consolidation
    print("\nInitiating sleep consolidation...")
    sleep = SleepConsolidation(model, memory, config)
    result = sleep.consolidate(n_replay=500, n_epochs=2, verbose=True)

    # Check results
    if result['status'] == 'success':
        print(f"\n‚úì Consolidation successful")
        print(f"  Memories pruned: {result['memories_pruned']}")
        print(f"  PC Loss: {result['pc_loss']:.4f}")
        print(f"  Reconstruction Loss: {result['reconstruction_loss']:.4f}")
        print(f"  Time: {result['consolidation_time']:.1f}s")

        # Success criteria: Memory was reduced, losses are reasonable
        memory_reduced = result['memories_pruned'] > 0
        losses_ok = result['pc_loss'] < 1.0 and result['reconstruction_loss'] < 1.0

        success = memory_reduced and losses_ok

        if success:
            print(f"\n‚úÖ SLEEP CONSOLIDATION: PASS")
            print(f"   Successfully consolidated with memory reduction")
        else:
            print(f"\n‚ö†Ô∏è  SLEEP CONSOLIDATION: PARTIAL")
            print(f"   Consolidation completed but metrics need improvement")
    else:
        print(f"\n‚ùå SLEEP CONSOLIDATION: FAIL")
        print(f"   Consolidation failed: {result.get('status')}")
        success = False

    return {
        'test': 'sleep_consolidation',
        'passed': success,
        'before_count': before_count,
        'after_count': memory.count,
        'memories_pruned': result.get('memories_pruned', 0),
        'consolidation_time': result.get('consolidation_time', 0),
    }


def run_all_benchmarks(config_name='small', device='cuda'):
    """Run all personalization benchmarks."""
    print("\n" + "="*80)
    print("EPISODIC-CENTRIC LLM PERSONALIZATION BENCHMARKS")
    print("="*80)

    # Initialize
    config = ConfigSmall()
    device = torch.device(device if torch.cuda.is_available() else 'cpu')

    print(f"\nConfiguration: {config_name}")
    print(f"Device: {device}")
    print(f"Model: {config.n_layers} layers, {config.embed_dim} dim")
    print(f"Memory: {config.memory_capacity} capacity")

    # Create model
    print(f"\nInitializing model...")
    model = PCModel(config=config).to(device)
    model.freeze()  # Freeze for personalization testing
    n_params = sum(p.numel() for p in model.parameters())
    print(f"‚úì Model initialized ({n_params/1e6:.1f}M params, frozen)")

    # Create memory
    print(f"Initializing episodic memory...")
    memory = OptimizedEpisodicMemory(
        dim=config.memory_dim,
        capacity=config.memory_capacity,
        use_faiss=True
    )
    print(f"‚úì Memory initialized")

    # Run tests
    results = []

    # Test 1: Instant Adaptation
    try:
        result = test_instant_adaptation(model, memory, config, device)
        results.append(result)
    except Exception as e:
        print(f"‚ùå Test failed with error: {e}")
        results.append({'test': 'instant_adaptation', 'passed': False, 'error': str(e)})

    # Test 2: Zero Forgetting
    try:
        result = test_zero_forgetting(model, memory, config, device, n_interactions=1000)
        results.append(result)
    except Exception as e:
        print(f"‚ùå Test failed with error: {e}")
        results.append({'test': 'zero_forgetting', 'passed': False, 'error': str(e)})

    # Test 3: Memory Efficiency
    try:
        result = test_memory_efficiency(model, memory, config, device)
        results.append(result)
    except Exception as e:
        print(f"‚ùå Test failed with error: {e}")
        results.append({'test': 'memory_efficiency', 'passed': False, 'error': str(e)})

    # Test 4: Sleep Consolidation
    try:
        result = test_sleep_consolidation(model, memory, config, device)
        results.append(result)
    except Exception as e:
        print(f"‚ùå Test failed with error: {e}")
        results.append({'test': 'sleep_consolidation', 'passed': False, 'error': str(e)})

    # Summary
    print("\n" + "="*80)
    print("BENCHMARK SUMMARY")
    print("="*80)

    passed = sum(1 for r in results if r['passed'])
    total = len(results)

    for result in results:
        status = "‚úÖ PASS" if result['passed'] else "‚ùå FAIL"
        print(f"{result['test']:30s} {status}")

    print(f"\nTotal: {passed}/{total} tests passed")

    if passed == total:
        print("\nüéâ ALL BENCHMARKS PASSED!")
        print("Episodic-centric personalization is working as expected.")
    else:
        print(f"\n‚ö†Ô∏è  {total - passed} benchmark(s) failed.")
        print("Review the results above for details.")

    return results


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description='Run personalization benchmarks')
    parser.add_argument('--config', type=str, default='small', choices=['small', 'micro'])
    parser.add_argument('--device', type=str, default='cuda')
    args = parser.parse_args()

    results = run_all_benchmarks(config_name=args.config, device=args.device)
